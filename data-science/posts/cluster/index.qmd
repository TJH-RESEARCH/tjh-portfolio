---
title: "Clustering for Dimension Reduction"
description: "Finding hidden patterns in social science data using clustering"
author:
  - name: Thomas Hodges
    orcid: 0000-0002-5184-7346
date: 2025-06-03
categories: 
  - Unsupervised Learning
draft: false
---

```{r, options, include=FALSE}
knitr::opts_chunk$set(
  comment = '', fig.width = 6, fig.height = 6,
  warning = FALSE, message = FALSE, cache = TRUE
)


```

## More data, more problems

> This post explains a data science project I have worked on. The complete [analysis code is available on GitHub](https://github.com/TJH-RESEARCH/identity-reintegration). A [paper detailing the analysis](../research/working-papers/identity-reintegration/index.qmd) is currently in the works. I presented a [poster based on this work](../../talks.index) at the Atlanta VA.

Ever heard the phrase "more money, more problems?" A similar idea exists in data science: more data, more problems. There are a few ways this comes about. Having too many cases (e.g., rows, observations) takes a lot of computing power, and can sometimes require different sets of tools and analyses. But having too many variables (e.g., columns, features) can also be a problem.

That's the case with a set of variables from a survey I collected with some veterans. I want to know how veteran identities affects a person's integration into civilian life. Conveniently, there is a validated measure of veterans identity: the "Warrior Identity Scale." Inconveniently, it measures 7 different aspects of military identity, each of which is considered a separate variable.

This presented me with the "curse of dimensionality," having more dimensions to the data than I can easily use. Several options were available. First, I could have used all 7 of the variables. Second, I could have chosen 1 or more of the variables as the most important variables. There are problems with both of these options. Using all the variables leads to a messier model. It has lots of predictor variables — all 7 of the Warrior Identity Scale measures — but it is hard to make sense of how those predictor variables are related. And we should expect some sort of relationship(s). After all, these are all aspects of the same phenomenon, identity.

The second option would be to select 1 or more of the identity variables. This is a problem to the extent that we arbitrarily pick which identity variables to include. We would ideally choose what variables to use by referencing a theory.

There is a third approach which is less dependent on theory and more data-driven: unsupervised learning to combine the 7 variables, reducing the 7 dimensions into 1. **Unsupervised learning** is a form of machine learning we can use to categorize data without having a "true" reference category. We are trying to uncover hidden patterns or "latent" groups.

A key issue with unsupervised learning is not having a ground truth to compare the model. No matter how good it fits the data you have, it is not possible to know if clusters corresponds to real groupings. In this project, I mitigated this issue by collecting an additional sample to see if the results would replicate.

## Unsupervised Learning

Clustering analysis can be used to address the "curse of dimensionality." It is one tool for "dimensional reduction," reducing the number of variables you have. Take the following 2 dimensional plot. Can you use it to spot groups in the data, reducing our two-dimensions to a 1-dimension categorical variable?

```{r cluster}
library(tidyverse) # for a suite of useful functions
library(palmerpenguins) # for example data

# For cool fonts
library(showtext)
font_add_google("Outfit", family = "special")
showtext_auto()

# For a reusable graph theme
theme_custom <-
  theme(
    text = element_text(family = "special"),
    plot.title = element_text(face = "bold", size = 18),
    plot.subtitle = element_text(face = "italic", size = 14),
    plot.caption = element_text(size = 16, hjust = -.01),
    axis.title = element_text(size = 10)
    )

# Create a base plot
plot_penguins <-
  penguins %>% 
  ggplot(aes(bill_depth_mm, bill_length_mm)) +
  labs(x = "Bill Depth (mm)", 
       y = "Bill Length (g)",
       subtitle = "Sizes of penguins from 3 different species") +
  theme_classic() +
  theme_custom

  
# Add points and labels to the plot. 
plot_penguins +
  geom_point(size = 2, alpha = .7) +
  labs(
    title = "Can you spot the groups in the data?", 
    caption = "Scroll down to see if you saw the correct groupings"
    )

```

```{r cluster-penguins}
plot_penguins + 
  geom_point(aes(color = species, 
                 shape = species),
             size = 2, 
             alpha = .7) + 
  labs(
    title = "Adding color and shape makes these groups easier to spot",
    color = "Penguin Species", 
    shape = "Penguin Species") +
  theme(legend.position = 'bottom')
  
```

While we cannot [easily visualize past two or three dimensions](https://www.data-to-viz.com/caveat/3d.html), mathematically we can use the same unsupervised learning tools in as many dimensions as we want. Instead of using human eyes, unsupervised learning algorithms use math to find the clusters.

Let's use one unsupervised learning tool to see if it can find the same 3 groups that we did. In R, we can do this with the `kmeans()` which is in base R's stats package. We will work with the same penguin data and see if the k-means algorithm can find the species groups (note: with unsupervised learning, we typically wouldn't try to spot groups that we already have labels for, like these penguin species in this toy data set):

```{r cluster-demo}
results <-
  penguins %>% 
  select(bill_length_mm, bill_depth_mm) %>% 
  filter(!is.na(bill_length_mm), !is.na(bill_depth_mm)) %>% 
  kmeans(centers = 3)

penguins <- 
  penguins %>% 
  filter(!is.na(bill_length_mm), !is.na(bill_depth_mm)) %>% 
  mutate(cluster = factor(results$cluster),
         correct = case_when(
           species == 'Gentoo' & cluster == 1 ~ 'correct',
           species == 'Gentoo' & cluster != 1 ~ 'incorrect',
           species == 'Chinstrap' & cluster == 2 ~ 'correct',
           species == 'Chinstrap' & cluster != 2 ~ 'incorrect',
           species == 'Adelie' & cluster == 3 ~ 'correct',
           species == 'Adelie' & cluster != 3 ~ 'incorrect',
           .default = NA)
         )

plot_penguins + 
  geom_point(data = penguins, aes(color = cluster, shape = correct)) + labs(
    title = "Did k-means clustering find the correct groups?",
    captions = "k-means worked well, although not perfectly.")

```

Besides k-means, there are other unsupervised algorithms we can use. These are:

1.  Hierarchical Clustering
2.  DBSCAN

-   What unsupervised learning method is best for my problem?

k-means (or related "centroid-based" clustering methods) work best when:

**1.** **Euclidean Distance**. For k-means to work best, the measures are related to each other in Euclidean distance. That is, the distance between two points should be in a standard n-dimension space. Just like the distance between two houses, or between your house and the moon. This would not apply to the distance between two nodes in a social network graph. In that case, the distance between nodes is not Euclidean but rather measured in degrees of Kevin Bacon separation.

A consequence of the Euclidean-distance-based algorithm is that the clusters found by k-means will be spherical. The

**2. Known number of clusters.** k-means is also best when you know the correct number of clusters. Most of the time, with unsupervised learning, we do not know the number of clusters.

Under the hood, the algorithm is trying to group the data to minimize within cluster variance. In other words, it tries to find the multidimensional distance from each data point to every other data point in its cluster.

One main issue with k-means is that it requires specifying how many clusters there are. In this case, we told k-means that there are 3 clusters, knowing that's how many species are in the data. But truly unsupervised learning would not know the appropriate number of clusters. In these cases, it may be more appropriate to use.

But it is still possible to use k-means for unsupervised learning, only we need to try different numbers of clusters and then compare these solutions.

The more clusters chosen, the lower the variance, but having the number of clusters equal to the number of data points (k = n) does us no help in reducing dimensions. Ultimately, we have to choose which solution (i.e., number of clusters) to choose, and we need to balance parsimony with fit. When we already know how many clusters to choose, this is not a difficult problem. But when we have no idea how many clusters are appropriate, we compare how well the different solutions fit the data.

-   How many clusters should I ask the k-means to find? Compare the fit of different solutions.

Hierarchical clustering provides a built-in solution to learning the best number of clusters.

DBSCAN, which stands for density-based spatial clustering of applications with noise, is best suited to data that have irregularly shaped clusters in a Euclidean space. In other words, things that are not groups shaped as spheres.

## Clustering Military Identity Data

Let's see how to compare different cluster solutions with the WIS veteran identity data.

First we will load some packages and set a seed number for the pseudo-random number generator. That will allow us to repeat the result. Finally, we will load the data. I've made this data set publicly available on GitHub. The code will allow you to download the data yourself and follow along.

```{r setup}
library(factoextra) # to visualize cluster
library(NbClust) # to visualize cluster fits

set.seed(10001) # to reproduce the results

data <- read_csv("data_main_replicate.csv") # download the data
```

Then we will standardize the data. Each variable used a standardized scale, but they have different ranges. It's probably not a big deal for this data, but we can standardize it anyways to be safe.

```{r standardize}
data_cluster_wis <- 
  data %>% 
  select(starts_with('wis') & ends_with('total') & !wis_total) %>% 
  scale()
```

Then we can use tools from the packages we loaded to compare different the number of clusters.

We will start with the elbow method. This is a graphic technique. We look to see where there is an "elbow" in the graph -- basically, where it bends the most. 

```{r elbow}
(plot_elbow <-  
fviz_nbclust(data_cluster_wis, kmeans, method = "wss") +
    geom_vline(xintercept = 3, linetype = 2) +
    labs(
      title = "The elbow method is a little ambiguous",
      subtitle = "But 3 appears to be the elbow"
      ) +
  theme_custom
)

```

The silhouette method is another graphical method for finding the best fitting number of clusters. Here we are looking for the greatest value on the y-axis.

```{r silhouette}
(plot_silhouette <-
  fviz_nbclust(data_cluster_wis, kmeans, method = "silhouette")+
    labs(title = "The silouette methods suggests 2 clusters is best") +
   theme_custom
)
```

The gap statistic is yet another method. This compares the fit of cluster solutions to the expected fit if the data were random. 


```{r gap}
(plot_gap_stat <-
  fviz_nbclust(data_cluster_wis, 
               kmeans, 
               nstart = 25,  
               print.summary = TRUE, 
               method = "gap_stat", 
               nboot = 500)  +
   labs(title = "Gap stat doesn't find a solution",
        subtitle = "The greatest gap stat is for 1 cluster",
        caption = "A 1-cluster solution is basically an inability to reject the null hypothesis that the data clusters into groups at all.") +
   theme_custom
)

```

We can try a suite of metrics, comparing a wide range of solutions. In this case, let's compare 2 to 15 groups.

```{r compare}
results_kmeans <- 
  NbClust(data = data_cluster_wis, 
        diss = NULL, 
        distance = "euclidean",
        min.nc = 2, 
        max.nc = 15, 
        method = 'kmeans',
        index = 'alllong')

```

Going by those results, the 2-group and 3- group solution look the best. Let's limit the number of clusters to compare head-to-head just those 2- and 3-group solutions. Actually, we can only compare

```{r compare-limited}
results_kmeans_limited = 
  NbClust(data = data_cluster_wis, 
            diss = NULL, 
            distance = "euclidean",
            min.nc = 2, 
            max.nc = 4, 
            method = 'kmeans',
            index = 'alllong')
```

## Validating the Clusters

A key issue with unsupervised learning is how to externally validate the clusters. How do we know if these classifications are real or just an artifact of the data?

In supervised learning, validation is easier. You use the model to make some predictions on data it wasn't trained on. Then you calculate how accurate the predictions were. With unsupervised learning, we have no idea how accurate we are. Instead, we need to validate the categories in different ways. In this project, I used two different validation methods.

First, the model should have some "predictive" validity. That is, the clusters should be able to predict other variables. For this project, veterans identity should predict how well they reintegrate into civilian society. Using the clusters as a predictor variable, I fit a linear regression model to a measure of reintegration.\
Second, the cluster groupings should be replicable in other data sets. That is, if we collect new data, we should find similar groupings if the grouping are real. With this in mind, I collected some more data with a new survey. Then, I repeated the clustering in the new data set.

If we choose the 3-cluster solution, we can inspect the groups. Let's plot the data and take a look.

```{r plots, eval = FALSE}
plot_profiles <-
  data %>% 
  mutate(wis_skills_total = wis_skills_total / 3,
         wis_centrality_total = wis_centrality_total / 4,
         wis_connection_total = wis_connection_total / 3,
         wis_family_total = wis_family_total / 3,
         wis_interdependent_total = wis_interdependent_total / 7,
         wis_private_regard_total = wis_private_regard_total / 7,
         wis_public_regard_total = wis_public_regard_total / 4) %>% 
  select(wis_skills_total,
         wis_centrality_total,
         wis_connection_total,
         wis_family_total,
         wis_interdependent_total,
         wis_private_regard_total,
         wis_public_regard_total,
         cluster) %>% 
  group_by(cluster) %>% 
  mutate(cluster = factor(cluster)) %>%
  summarise(across(everything(), ~ mean(.x))) %>% 
  pivot_longer(!cluster) %>% 
  ggplot(
    aes(
      name, 
      value, 
      color = cluster,
      group = cluster)) + 
  geom_point(
    aes(
      shape = cluster), 
    size = 4) + 
  geom_line(
    aes(
      linetype = cluster)) +
  scale_color_manual(values = c('#440154', '#3b528b', '#5ec962')) +
  #scale_color_viridis(option = 'C', discrete = TRUE) +
  theme_classic() +
  labs(title = paste0('Military Identity by Latent Groups'), 
       x = 'Aspect of Military Identity', 
       y = 'Identity (mean item score)') + 
  theme(axis.text.x = element_text(angle = -30, vjust = 1, hjust = 0)) +
  labs(color = 'Cluster',
       shape = 'Cluster',
       linetype = 'Cluster') +
  scale_x_discrete(
  labels = c('Centrality', 
             'Connection',
             'Family',
             'Interdependence',
             'Private Regard',
             'Public Regard',
             'Skills')
  ) +
  theme(axis.text = element_text(size = 11),
        text = element_text(size = 14))
```
