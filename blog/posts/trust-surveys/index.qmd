---
title: "Why your employees aren’t engaging with your employee engagement survey"
description: "Why it’s a problem and what you can do about it"
author:
  - name: Thomas Hodges
    orcid: 0000-0002-5184-7346
    affiliation: Center for the Advancement of Military and Emergency Services Research, Kennesaw State University
affiliation-url: https://www.kennesaw.edu/research/centers-facilities/ames-center/index.php
date: 2025-06-11
#categories: \[R, Statistics, Research Design\] \# self-defined categories
#citation: 
#  url: https://samanthacsik.github.io/posts/2022-10-24-my-blog-post/ 
#image: 
draft: true
---

> Tl;dr: they don’t trust that your survey is truly anonymous, they think you’ll use it against them, and they might be right. I’ll tell you how to solve that.

People don’t trust surveys. 

Employees think they are being spied on. They think an engagement survey is an elaborate trap to find people who are not committed to the organization. For certain occupations, the fear is even worse. For example, police officers might worry that a mental health survey will be used to label them as unfit for duty. 

Even outside of the workplace, people might fear their data is used to label  them as problematic. Or with public polling surveys, like the ones used in the run up to an election, people can [mistrust pollsters](https://www.wnycstudios.org/podcasts/otm/segments/can-we-trust-polls-on-the-media) or the news media that will report on the poll. 

The good news is that these fears are mostly unfounded. People analytics teams want to improve workplaces, not surveil deviant workers. When we run health surveys at the [AMES Research Center](https://www.kennesaw.edu/research/centers-facilities/ames-center/), we use to data to tailor services for our client’s specific needs. 

Still, survey fears are problem for survey admins trying to do the right thing. Fear can lead to low participation. It can also cause participants to lie on their survey. Or they might respond carelessly in other ways, such as "straightlining" or "Christmas treeing." 

One easy workaround is to make surveys anonymous. The survey’s don’t ask for your name, so how can they be used to track you and punish you? This poses a problem, however, for longitudinal surveys: those where you want to survey the same people at repeated points over time. 

In this post I’ll tell you how we solve these problems of trust, anonymity, and data qualiity in our work with organizations and communities and the problems we still face. I will also demonstrate how to clean bad data with R using the [careless package](https://github.com/ryentes/careless) and why linking surveys over time helps your statistical analysis.

## Is your survey really anonymous?

Whenever possible, you should make your surveys anonymous. Truly anonymous. To start, this means you should not ask participants their name. But there are other ways to identify responses. Survey hosts such as [Qualtrics](https://www.qualtrics.com/support/getting-started-qualtrics/minimizing-personal-data-collection-and-use-in-qualtrics/) collect IP Address, Latitude, and Longitude by default, even for its "anonymous" QR code or link. You can and should disable this. Then you should tell your participants that you disabled them. Build trust by being open and transparent and by making strong claims about how you are protecting their privacy.

The problem is a bit more difficult. Depending on the number of people in your organization, the questions you ask can narrow down respondents. This is especially true for demographic questions. To demonstrate this problem, let's simulate a survey using R.

Assume there are two teams in your organization, marketing and accounting. Each team has 3 people, and half of the employees are men and half are women. First we will create this data:

```{r}
library(tidyverse) # Load this package for convenient R functions

## Create the data:
(employees <-
  tibble(
    employee = c("Tom", "Thula", "Tim", "Paulina", "Pedro", "Priscilla"),
    gender = rep(c('Male', "Female"), 3),
    team = c(rep("Marketing", 3), rep("Accounting", 3))
  )
)

```

Now let's administer a survey. Assume there is a 50% chance of employees responding. On the survey, we ask people what team they work on and what their gender is.

```{r}
## make it random, but use this "seed" to replciate my results
set.seed(369)

(responses <-
employees %>% 
  mutate(
    respond = sample(c("Yes", "No"), size = 6, replace = TRUE, prob = c(.50, .50))
  ) %>% 
  select(-employee) %>% 
  filter(respond == "Yes")
)
```

In these results, the first row is either Tom or Tim, the two males in marketing. There is a 50% chance of it being either other of them. The second row is either Paulina or Priscilla with a 50% chance. The third row is 100% Pedro, the only guy in accounting.

Let's calculate these percentages in R so we can scale up the simulation. To do so, we will need to add the number of people per team, and the number of males/females per team.

```{r}

## First calculate the team size
(employees <-
  employees %>% 
  mutate(
    female = ifelse(gender == "Female", 1, 0),
  ) %>% 
  group_by(team) %>% 
  mutate(team_size = n(), n_females = sum(female)) %>% 
  ungroup()
)
```

Now add the percentages.

```{r}
(employees <-
  employees %>% 
  mutate(
    perc = 
      ifelse(female == 1, 
           1 / n_females,
           1 / (team_size - n_females)
           )
  )
)
```

Let's wrap these scripts in a few functions to reuse them.

```{r}

create_data <- function(team_size) {
  employees <-
  tibble(
    employee_id = c(1:(team_size * 2)),
    gender = rep(c('Male', "Female"), team_size),
    team = c(rep("Marketing", team_size), rep("Accounting", team_size))
  )
  return(employees)
}

calculate_perc <-
  function(data){
    
    data <-
    data %>% 
    mutate(female = ifelse(gender == "Female", 1, 0)) %>% 
    group_by(team) %>% 
    mutate(team_size = n(), n_females = sum(female)) %>% 
    mutate(perc = ifelse(female == 1, 1 / n_females, 1 / (team_size - n_females))) %>% 
    ungroup()
    
    return(data)
  }

run_survey <- function(data){
  responses <-
    data %>% 
    mutate(
      respond = sample(c("Yes", "No"), size = nrow(.), replace = TRUE, prob = c(.50, .50))
    ) %>% 
    filter(respond == "Yes")
  
  return(responses)
}
```

With out function ready, let's increase the size of the teams from 3 to 7, then add our percentages, and run our survey.

```{r}
(responses <-
  create_data(team_size = 7) %>% 
  calculate_perc() %>% 
  run_survey()
)
```

Notice that men and women are distributed as evenly as possible, although the odd-number team size means that the minority gender on a team has a higher chance of identification. With even number teams, and an evenly split gender balance, the chance of identification is equal.:

```{r}
(responses <- create_data(team_size = 4) %>% calculate_perc() %>% run_survey())
```

Apart from parity, there is also safety in large numbers, such as 100-person teams:

```{r}
(responses <- create_data(team_size = 100) %>% calculate_perc() %>% run_survey())
```

## The problem with anonymous surveys for longitudinal surveys

## 
