{
  "hash": "62ce40c182c9a749dbf8d061f6627108",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Clustering for Dimension Reduction\"\ndescription: \"Finding hidden patterns in social science data using clustering\"\nauthor:\n  - name: Thomas Hodges\n    orcid: 0000-0002-5184-7346\ndate: 2025-06-03\ncategories: \n  - Unsupervised Learning\ndraft: false\n---\n\n\n\n\n\n\n> This post explains a data science project I have worked on. The complete [analysis code is available on GitHub](https://github.com/TJH-RESEARCH/identity-reintegration), but you can also find some code snippets in this post. A [paper detailing the analysis](../../../research/working-papers/identity-reintegration/index.qmd) is currently in the works. I presented a [poster based on this work](../../../talks/index.qmd) at the Atlanta VA on May 16, 2024.\n\n## More data, more problems\n\nEver heard the phrase \"more money, more problems?\" A similar idea exists in data science: more data, more problems. There are a few ways this comes about. Having too many cases (e.g., rows, observations) takes a lot of computing power, and can sometimes require different sets of tools and analyses. But having too many variables (e.g., columns, features) can also be a problem.\n\nThat's the case with a set of variables from a survey I collected with some veterans. I want to know how veteran identities affects a person's integration into civilian life. Conveniently, there is a validated measure of veterans identity: the \"Warrior Identity Scale.\" Inconveniently, it measures 7 different aspects of military identity, each of which is considered a separate variable.\n\nThis presented me with the \"curse of dimensionality,\" having more dimensions to the data than I can easily use. Several options were available. First, I could have used all 7 of the variables. Second, I could have chosen 1 or more of the variables as the most important variables. There are problems with both of these options. Using all the variables leads to a messier model. It has lots of predictor variables — all 7 of the Warrior Identity Scale measures — but it is hard to make sense of how those predictor variables are related. And we should expect some sort of relationship(s). After all, these are all aspects of the same phenomenon, identity.\n\nThe second option would be to select 1 or more of the identity variables. This is a problem to the extent that we arbitrarily pick which identity variables to include. We would ideally choose what variables to use by referencing a theory.\n\nThere is a third approach which is less dependent on theory and more data-driven: unsupervised learning to combine the 7 variables, reducing the 7 dimensions into 1. **Unsupervised learning** is a form of machine learning we can use to categorize data without having a \"true\" reference category. We are trying to uncover hidden patterns or \"latent\" groups.\n\nA key issue with unsupervised learning is not having a ground truth to compare the model. No matter how good it fits the data you have, it is not possible to know if clusters corresponds to real groupings. In this project, I mitigated this issue by collecting an additional sample to see if the results would replicate.\n\n## Unsupervised Learning\n\nClustering analysis can be used to address the \"curse of dimensionality.\" It is one tool for \"dimensional reduction,\" reducing the number of variables you have. Take the following 2 dimensional plot. Can you use it to spot groups in the data, reducing our two-dimensions to a 1-dimension categorical variable?\n\n\n\n\n\n\n\n\n\n\n\nWhile we cannot [easily visualize past two or three dimensions](https://www.data-to-viz.com/caveat/3d.html), mathematically we can use the same unsupervised learning tools in as many dimensions as we want. Instead of using human eyes, unsupervised learning algorithms use math to find the clusters.\n\nLet's use one unsupervised learning tool to see if it can find the same 3 groups that we did. In R, we can do this with the `kmeans()` which is in base R's stats package. We will work with the same penguin data and see if the k-means algorithm can find the species groups (note: with unsupervised learning, we typically wouldn't try to spot groups that we already have labels for, like these penguin species in this toy data set):\n\n\n\n\n\n\n\n\n\nBesides k-means, there are other unsupervised algorithms we can use. These are:\n\n1.  Hierarchical Clustering\n2.  DBSCAN\n\n### What unsupervised learning method is best for my problem?  \n\nk-means (or related \"centroid-based\" clustering methods) work best when:\n\n**1.** **Euclidean Distance**. For k-means to work best, the measures are related to each other in Euclidean distance. That is, the distance between two points should be in a standard n-dimension space. Just like the distance between two houses, or between the earth and the moon. This would **not** apply to the distance between two nodes in a social network graph. In that case, the distance between nodes is not Euclidean but rather measured in degrees of Kevin Bacon separation.\n\nA consequence of the Euclidean-distance-based algorithm is that the clusters found by k-means will be spherical. Just like planets in the orbit of the sun, the data points for each group will be spaced out around a center (the \"mean\" in k-means). \n\n**2. Known number of clusters.** k-means is also best when you know the correct number of clusters. Most of the time, with unsupervised learning, we do not know the number of clusters.\n\nUnder the hood, the algorithm is trying to group the data to minimize within cluster variance. In other words, it tries to find the multidimensional distance from each data point to every other data point in its cluster.\n\nOne main issue with k-means is that it requires specifying how many clusters there are. In this case, we told k-means that there are 3 clusters, knowing that's how many species are in the data. But truly unsupervised learning would not know the appropriate number of clusters. In these cases, it may be more appropriate to use.\n\nBut it is still possible to use k-means for unsupervised learning, only we need to try different numbers of clusters and then compare these solutions.\n\nThe more clusters chosen, the lower the variance, but having the number of clusters equal to the number of data points (k = n) does us no help in reducing dimensions. Ultimately, we have to choose which solution (i.e., number of clusters) to choose, and we need to balance parsimony with fit. When we already know how many clusters to choose, this is not a difficult problem. But when we have no idea how many clusters are appropriate, we compare how well the different solutions fit the data.\n\nHierarchical clustering provides a built-in solution to learning the best number of clusters.\n\nDBSCAN, which stands for density-based spatial clustering of applications with noise, is best suited to data that have irregularly shaped clusters in a Euclidean space. In other words, things that are not groups shaped as spheres.\n\n## Clustering Military Identity Data\n\nLet's see how to compare different cluster solutions with the WIS veteran identity data that I told you about at the top of this post. Remember, there are 7 variables, and that's too many. We need to reduce the number of dimensions with clustering. \n\nLet me walk you through the R code for the analysis. First we will load some packages and set a seed number for the pseudo-random number generator. That will allow us to repeat the result. Finally, we will load the data. I've made this data set publicly available on GitHub. The code will allow you to download the data yourself and follow along.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(factoextra) # to visualize cluster\nlibrary(NbClust) # to visualize cluster fits\n\nset.seed(10001) # to reproduce the results\n\ndata <- read_csv(\"data_main_replicate.csv\") # download the data\n```\n:::\n\n\n\n\nThen we will standardize the data. Each variable used a standardized scale, so it is probably not a big deal for this data, but they do have different ranges, so let's standardize it anyways to be safe.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata_cluster_wis <- \n  data %>% \n  select(starts_with('wis') & ends_with('total') & !wis_total) %>% \n  scale()\n```\n:::\n\n\n\n\nThen we can use tools from the packages we loaded to compare different the number of clusters.\n\nWe will start with the elbow method. This is a graphical technique. We look to see where there is an \"elbow\" in the graph -- that is, where the line bends the most.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n(plot_elbow <-  \nfviz_nbclust(data_cluster_wis, kmeans, method = \"wss\") +\n    geom_vline(xintercept = 3, linetype = 2) +\n    labs(\n      title = \"The elbow method is a little ambiguous\",\n      subtitle = \"But 3 appears to be the elbow\"\n      ) + theme_custom\n)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/elbow-1.png){width=576}\n:::\n:::\n\n\n\n\nThe silhouette method is another graphical method for finding the best fitting number of clusters. Here we are looking for the greatest value on the y-axis.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n(plot_silhouette <-\n  fviz_nbclust(data_cluster_wis, kmeans, method = \"silhouette\")+\n    labs(title = \"The silouette method favors 2 clusters\") +\n  theme_custom\n)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/silhouette-1.png){width=576}\n:::\n:::\n\n\n\n\nThe gap statistic is yet another method. This compares the fit of cluster solutions to the expected fit if the data were random.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n(plot_gap_stat <-\n  fviz_nbclust(data_cluster_wis, \n               kmeans, \n               nstart = 25,  \n               print.summary = TRUE, \n               method = \"gap_stat\", \n               nboot = 500)  +\n   labs(title = \"The gap stat can't find a solution with this data\",\n        subtitle = \"The greatest gap stat is for 1 cluster\",\n        caption = \"A 1-cluster solution isn't really a cluster\") +\n   theme_custom\n)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/gap-1.png){width=576}\n:::\n:::\n\n\n\n\nThe gap statistic was not very helpful. It suggests a 1-cluster solution, which isn't really a cluster, now is it? Another way of thinking about this 1-cluster gap stat is an inability to reject the null hypothesis that the data clusters into groups at all.\n\nThe graphical methods are interesting. At least they give us something to look at. But there is another way to find the best number of clusters. \n\nUsing the [`NbClust`](https://www.jstatsoft.org/article/view/v061i06) package, we can compare lots of different cluster solutions with a suite of 30 metrics. \n\nIn this case, let's compare solutions from 2 to 15 groups.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nresults_kmeans <- \n  NbClust(data = data_cluster_wis, \n        diss = NULL, \n        distance = \"euclidean\",\n        min.nc = 2, \n        max.nc = 15, \n        method = 'kmeans',\n        index = 'alllong')\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/compare-1.png){width=576}\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n*** : The Hubert index is a graphical method of determining the number of clusters.\n                In the plot of Hubert index, we seek a significant knee that corresponds to a \n                significant increase of the value of the measure i.e the significant peak in Hubert\n                index second differences plot. \n \n```\n\n\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/compare-2.png){width=576}\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n*** : The D index is a graphical method of determining the number of clusters. \n                In the plot of D index, we seek a significant knee (the significant peak in Dindex\n                second differences plot) that corresponds to a significant increase of the value of\n                the measure. \n \n******************************************************************* \n* Among all indices:                                                \n* 11 proposed 2 as the best number of clusters \n* 9 proposed 3 as the best number of clusters \n* 1 proposed 4 as the best number of clusters \n* 1 proposed 13 as the best number of clusters \n* 2 proposed 14 as the best number of clusters \n* 3 proposed 15 as the best number of clusters \n\n                   ***** Conclusion *****                            \n \n* According to the majority rule, the best number of clusters is  2 \n \n \n******************************************************************* \n```\n\n\n:::\n:::\n\n\n\n\nGoing by those results, the 2-group and 3- group solution look the best. Let's limit the number of clusters to compare head-to-head just those 2- and 3-group solutions. Actually, `NbClust` makes us compare at least three different solutions, so we will compare the 2-, 3-, and 4-group solutions.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nresults_kmeans_limited = \n  NbClust(data = data_cluster_wis, \n            diss = NULL, \n            distance = \"euclidean\",\n            min.nc = 2, \n            max.nc = 4, \n            method = 'kmeans',\n            index = 'alllong')\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/compare-limited-1.png){width=576}\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n*** : The Hubert index is a graphical method of determining the number of clusters.\n                In the plot of Hubert index, we seek a significant knee that corresponds to a \n                significant increase of the value of the measure i.e the significant peak in Hubert\n                index second differences plot. \n \n```\n\n\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/compare-limited-2.png){width=576}\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n*** : The D index is a graphical method of determining the number of clusters. \n                In the plot of D index, we seek a significant knee (the significant peak in Dindex\n                second differences plot) that corresponds to a significant increase of the value of\n                the measure. \n \n******************************************************************* \n* Among all indices:                                                \n* 13 proposed 2 as the best number of clusters \n* 10 proposed 3 as the best number of clusters \n* 4 proposed 4 as the best number of clusters \n\n                   ***** Conclusion *****                            \n \n* According to the majority rule, the best number of clusters is  2 \n \n \n******************************************************************* \n```\n\n\n:::\n:::\n\n\n\n\n## Validating the Clusters\n\nA key issue with unsupervised learning is how to externally validate the clusters. How do we know if these classifications are real or just an artifact of the data?\n\nIn supervised learning, validation is easier. You use the model to make some predictions on data it wasn't trained on. Then you calculate how accurate the predictions were. With unsupervised learning, we have no idea how accurate we are. Instead, we need to validate the categories in different ways. In this project, I used two different validation methods.\n\nFirst, the model should have some \"predictive\" validity. That is, the clusters should be able to predict other variables. For this project, veterans identity should predict how well they reintegrate into civilian society. Using the clusters as a predictor variable, I fit a linear regression model to a measure of reintegration.\\\nSecond, the cluster groupings should be replicable in other data sets. That is, if we collect new data, we should find similar groupings if the grouping are real. With this in mind, I collected some more data with a new survey. Then, I repeated the clustering in the new data set.\n\nIf we choose the 3-cluster solution, we can inspect the groups. Let's plot the data and take a look.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot_profiles <-\n  data %>% \n  mutate(wis_skills_total = wis_skills_total / 3,\n         wis_centrality_total = wis_centrality_total / 4,\n         wis_connection_total = wis_connection_total / 3,\n         wis_family_total = wis_family_total / 3,\n         wis_interdependent_total = wis_interdependent_total / 7,\n         wis_private_regard_total = wis_private_regard_total / 7,\n         wis_public_regard_total = wis_public_regard_total / 4) %>% \n  select(wis_skills_total,\n         wis_centrality_total,\n         wis_connection_total,\n         wis_family_total,\n         wis_interdependent_total,\n         wis_private_regard_total,\n         wis_public_regard_total,\n         cluster) %>% \n  group_by(cluster) %>% \n  mutate(cluster = factor(cluster)) %>%\n  summarise(across(everything(), ~ mean(.x))) %>% \n  pivot_longer(!cluster) %>% \n  ggplot(\n    aes(\n      name, \n      value, \n      color = cluster,\n      group = cluster)) + \n  geom_point(\n    aes(\n      shape = cluster), \n    size = 4) + \n  geom_line(\n    aes(\n      linetype = cluster)) +\n  scale_color_manual(values = c('#440154', '#3b528b', '#5ec962')) +\n  #scale_color_viridis(option = 'C', discrete = TRUE) +\n  theme_classic() +\n  labs(title = paste0('Military Identity by Latent Groups'), \n       x = 'Aspect of Military Identity', \n       y = 'Identity (mean item score)') + \n  theme(axis.text.x = element_text(angle = -30, vjust = 1, hjust = 0)) +\n  labs(color = 'Cluster',\n       shape = 'Cluster',\n       linetype = 'Cluster') +\n  scale_x_discrete(\n  labels = c('Centrality', \n             'Connection',\n             'Family',\n             'Interdependence',\n             'Private Regard',\n             'Public Regard',\n             'Skills')\n  ) +\n  theme(axis.text = element_text(size = 11),\n        text = element_text(size = 14))\n```\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}