{
  "hash": "dd06e88c8f67cf102816a781fd64e526",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Cleaning and Screening Messy Real-World Survey Data\"\ndescription: \"You analyze the data you have, not the data you want\"\nauthor:\n  - name: Thomas Hodges\n    orcid: 0000-0002-5184-7346\ndate: 2025-06-04\ncategories: \n  - Data Cleansing\ndraft: false\n---\n\n\n\n## Project Summary\n\nI have conducted dozens of surveys with employees, students, and community members. A common issue with surveys is data quality. People skip answers, they lie, they straight-line, they respond in a random pattern, they take the wrong version of surveys. I've seen it all.\n\nLuckily there are tools to screen and clean bad survey data. For this project, I used cutting-edge methods to detect poor-quality survey responses. I then repeated the process for two additional surveys.\n\nThe results are clean data sets that have been used for several scientific publications. Additionally, I am drafting a paper that outlines the methods in an accessible way.\n\nFind the complete analysis code on GitHub and a walkthrough below.\n\n## Skills Demonstrated\n\n1.  R Programming\n2.  Data Quality Assurance\n3.  Survey Design and Analysis\n\n## Real-World Applications:\n\n-   People Analytics\n\n-   Employee Health and Well-being\n\n-   Employee Engagement and Satisfaction surveys\n\n## Walkthrough\n\nWhat's the problem: Real-World\n\nLet me walk you through the problem and solution. Imagine you just completed a survey with your employees, customers, or another group of stakeholders. Here I have a survey with a group of veterans. You might be quick to import your data and start to analyze it. Using R, let's download the data and look at it:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse,quietly = TRUE)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.2     ✔ tibble    3.3.0\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.4     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\n```\n\n\n:::\n\n```{.r .cell-code}\ndata <- read_csv(\"data/veterans.csv\")\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nRows: 234 Columns: 371\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (21): StartDate, EndDate, IPAddress, RecordedDate, ResponseId, Distribu...\ndbl (343): Status, Progress, Duration (in seconds), Finished, LocationLatitu...\nlgl   (7): RecipientLastName, RecipientFirstName, RecipientEmail, ExternalRe...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n```\n\n\n:::\n\n```{.r .cell-code}\ndata %>% select(StartDate, ResponseId, pcl_1, pcl_2, pcl_3) %>% slice_head(n = 7)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 7 × 5\n  StartDate     ResponseId        pcl_1 pcl_2 pcl_3\n  <chr>         <chr>             <dbl> <dbl> <dbl>\n1 3/21/25 18:13 R_6XDY8zzzNddbmvv     1     0     0\n2 3/21/25 18:20 R_3hLIDnq6C1KR4U0     3     3     0\n3 3/21/25 18:27 R_394gR6v6FG3xC18     0     0     0\n4 3/21/25 18:24 R_7pc0ZTcbhv4yF9Q     1     0     1\n5 3/21/25 18:20 R_1doiI3icHoB6bqV     2     1     0\n6 3/21/25 18:11 R_5qadE6upLOH6fQK     4     4     3\n7 3/21/25 18:47 R_3qaVN3lueCeTLVo     0     0     0\n```\n\n\n:::\n:::\n\n\n\nWe have 234 rows. In this case, that's supposed to be 234 US military veterans who each responded to the survey 1 time.\n\nThe participants were recruited online, so we cannot be sure that these are all veterans. To ensure these people we veterans, we asked two questions. To participate, you have to have previously served in the military, but not currently be on active duty:\n\n1.  Did you serve in the United States military?\n\n2.  Are you now separated from the military? (e.g., discharged, released from active duty, transferred to the Inactive Ready Reserve)\n\n\n\n    ::: {.cell}\n    \n    ```{.r .cell-code}\n    data %>% count(military_screener)\n    ```\n    \n    ::: {.cell-output .cell-output-stdout}\n    \n    ```\n    # A tibble: 2 × 2\n      military_screener     n\n                  <dbl> <int>\n    1                 0     1\n    2                 1   233\n    ```\n    \n    \n    :::\n    \n    ```{.r .cell-code}\n    data %>% count(veteran_screener)\n    ```\n    \n    ::: {.cell-output .cell-output-stdout}\n    \n    ```\n    # A tibble: 3 × 2\n      veteran_screener     n\n                 <dbl> <int>\n    1                0    19\n    2                1   214\n    3               NA     1\n    ```\n    \n    \n    :::\n    :::\n\n\n\nLuckily, no one ever lies on the internet. But let's assume they did. Maybe there are people out there who spend all day looking for surveys to take. Maybe that's because some surveys offer money or compensation for responses.\n\nHow can we tell who is legit? We can build some security into the survey with a \"validity check.\" We ask a question that only a legit respondent knows. Or, we ask a series of questions and test if people are logically consistent.\n\nIn this case, we asked participants which military rank was higher, and provided them with two military ranks. For anyone in the military, it should be immediately obvious which rank is higher and lower. For people outside of the military, this is a little more difficult. not impossible, but not immediately obvious. That's perfect. We want it to be so easy that 100% of legit people can answer it accurately. This doesn't mean the liers can't answer it correctly, but it does give us one way to weed them out.\n\n> Which military rank is higher, private or lieutenant?\n\nHow many people got the answer wrong?:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata %>% \n  filter(!is.na(validity_check)) %>% \n  count(validity_check) %>% \n  mutate(perc = n / sum(n))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 3\n  validity_check     n  perc\n           <dbl> <int> <dbl>\n1              0    14  0.07\n2              1   186  0.93\n```\n\n\n:::\n:::\n\n\n\n14 of the 200 people who said they were veterans did not answer a simple question testing their basic military knowledge. For an online survey, that's grounds for screening your response. Let's go ahead and filter based on the screening questions and validity checks.:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata <-\n  data %>% \n  filter(\n    age_screener == 1, \n    military_screener == 1, \n    veteran_screener == 1, \n    validity_check == 1\n  )\n\ndata %>% nrow()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 186\n```\n\n\n:::\n:::\n\n\n\nWe can also test for logical inconsistencies or impossibilities.\n\nFor example, we might compare the rank that participants claimed to have achieved with the number of years they claimed to have served. If they report a really high rank in too few years, then they are lying about something. For example, it is VERY impressible although not impossible to reach the rank of E-7 in 7 years. This is the so-called \"7 in 7.\" Anyone who claims 7 in 7 on an online is probably lying, but they might be telling the trust. But anyone who claims to have reached 7 in fewer years is almost certainly lying. Did we have this problem in our data?\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata %>%\n  filter(pay_grade == \"E-7 to E-9\") %>% \n  ggplot(aes(years_served)) +\n  geom_histogram() + \n  labs(x = 'Years Served', y = \"n\", \"Pay Grade E-7 to E-9\") + \n  theme_minimal()\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n:::\n\n\n\nAs a final note, you should also check the bot detection, duplicate, and fraud stats that are available from survey hosts like Qualtrics: \"A score of Less than 0.5 means the respondent is likely a bot.\" https://www.qualtrics.com/support/survey-platform/survey-module/survey-checker/fraud-detection/#RelevantID\n\n\"A score greater than or equal to 30 means the response is likely fraudulent and a bot.\"\n\nA score of greater than or equal to 75 means the response is likely a duplicate. https://www.qualtrics.com/support/survey-platform/survey-module/survey-checker/fraud-detection/#RelevantID )\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#Q_RecaptchaScore >= .5\ndata %>% \n  ggplot(aes(Q_RecaptchaScore)) + \n  geom_histogram() + \n  labs(title = \"Bots\", x = \"ReCAPTCHA Score\", y = \"n\")\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n#Q_RelevantIDDuplicateScore < 75\ndata %>% \n  ggplot(aes(Q_RelevantIDFraudScore)) + \n  geom_histogram() +\n  labs(title = \"Frauds\", x = \"Fraud Score\", y = \"n\")\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n#Q_RelevantIDFraudScore < 30\ndata %>% \n  ggplot(aes(Q_RelevantIDDuplicateScore)) + \n  geom_histogram() + \n  labs(title = \"Duplicates\", x = \"Duplicate Score\", y = \"n\")\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n:::\n\n\n\n### Dirty Data: Careless survey responding. Examples using R visualizations\n\nEven once you screen out the cheaters and bots, survey data can still have problems. People skip answers, they lie, they straight-line, they respond in a random pattern, they take the wrong version of surveys. Depending how the data was collected, some of these problems can be worse than others.\n\nWe can detect two kinds of careless responding: longstringing and random responding.\n\n#### Longstringing\n\nThere are 4 kinds of straightlining to watch out for. This is a combination of 2 factors: longest string or average string; and reverse-coded or not.\n\nFirst, there are two ways to calculate long strings, either as the longest string legnth or as the average string length. The average string is useful to detect careless responders who occasionally vary their response. The longest string is useful to catch people who do not vary their response in the slightest.\n\nSecond, longstring can be calucaled on data that has been reverse scored or that has not yet been reverse scored. The idea here is that some respondents who long string might pay attention to the valence of the question, looking for times that the wording switches from positive to negative, and varying their responses in accordance. Therefore, it might sense to calculate longstring with both reverse-scored and non-reverse scored answers. Each can be used to detect a slightly different type of careless survey responding behavior.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Longstring\n\n## Calculate longstring and average string without reverse scoring\ndata <-\n  data %>% \n  select(biis_1:cos_16) %>%         # select only the scale items after PCL and MIOS\n  transmute(\n    longstring = careless::longstring(.),\n    longstring_avg = careless::longstring(., avg = TRUE)[,2]\n  ) %>% \n  bind_cols(data)\n\n## Plot the long strings. Without the previous removals, there are 3 outliers for longstring (>15) and one for average string (>5)\ndata %>% ggplot(aes(longstring)) + geom_histogram()\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n\n```{.r .cell-code}\ndata %>% ggplot(aes(longstring_avg)) + geom_histogram()\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-6-2.png){width=672}\n:::\n:::\n\n\n\n#### Random Responding\n\nAlso known as Christmas-Treeing, to respond randomly to a survey means to select each answer at random, without regard for answering truthfully.\n\nWhile there might be a few reasons that people respond randomly to a survey, one of the main reason is probably to finish the survey quickly. Additionally, they may seek to avoid detection for their carelessness, thus opting not to use straightlining.\n\nThe tools to detect random responding are more varied than straightlining.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## Arrange the data and calculate the metric\ndata <-\n  data %>% \n  select(mios_1:cos_16) %>%         # select only the scale items\n  select(-mios_9) %>%              # issue with original did not include all 14 items\n  select(-starts_with('bipf')) %>%  # Drop the inventory\n  select(\n         # MIOS Shame\n         mios_1, \n         mios_3, \n         mios_7, \n         mios_8, \n         mios_12, \n         mios_13, \n         mios_14,\n         \n         # MIOS Trust\n         mios_2,\n         mios_4,\n         mios_5,\n         mios_6,\n         #mios_9,\n         mios_10,\n         mios_11,\n         \n         # DRDI Dysfunction\n         drdi_2,\n         drdi_4,\n         drdi_5,\n         drdi_8,\n         drdi_10,\n         drdi_13,\n         drdi_14,\n         \n         # DRDI recovery\n         drdi_1,\n         drdi_3,\n         drdi_6,\n         drdi_7,\n         drdi_9,\n         drdi_11,\n         drdi_12,\n         everything()\n  ) %>% \n  transmute(evenodd = \n              careless::evenodd(x =., \n                                # nItems in each subscale in order:\n                                factors = c(\n                                  \n                                  7,  # MIOS Shame\n                                  6,  # MIOS Trust\n                                  7,  # DRDI Dysfunction\n                                  7,  # DRDI Recovery\n                                  20, # PCL\n                                  10, # BIIS Harmony\n                                  7,  # BIIS Blended\n                                  6,  # MCARM Purpose \n                                  4,  # MCARM Help\n                                  3,  # MCARM Civilians\n                                  3,  # MCARM Resentment\n                                  5,  # MCARM Regimentation\n                                  7,  # WIS Private Regard\n                                  7,  # WIS Interdependent\n                                  3,  # WIS Connection\n                                  3,  # WIS Family\n                                  4,  # WIS Centrality\n                                  4,  # WIS Public Regard\n                                  3,  # WIS Skills\n                                \n                                 12,  # SCC \n                                  6,  # MPSS\n                                  8,  # SSS\n                                  8,  # COS\n                                  8)  # COS\n                                \n              )\n            ) %>% bind_cols(data) # Add the results back to the original data. \n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: There was 1 warning in `transmute()`.\nℹ In argument: `evenodd = careless::evenodd(...)`.\nCaused by warning in `careless::evenodd()`:\n! Computation of even-odd has changed for consistency of interpretation\n          with other indices. This change occurred in version 1.2.0. A higher\n          score now indicates a greater likelihood of careless responding. If\n          you have previously written code to cut score based on the output of\n          this function, you should revise that code accordingly.\n```\n\n\n:::\n\n```{.r .cell-code}\n## Plot the even-odd correlation. The outliers are around -.50, but that is a fairly strong correlation, which indicates careful responding. I wont remove any based on this. \ndata %>% ggplot(aes(evenodd)) + geom_histogram()\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n:::\n\n\n\n### Cutpoints\n\nData science is full of decisions. Some people don't want you to know that. They want you think that data is objective. That data speaks for itself. That the numbers don't lie.\n\nIt's true that data can be used to make more objective decisions, but that doesn't mean a whole lot of (potentially subjective) decisions have to be made just to analyze the data in the first place.\n\nWhen it comes to screening and cleaning survey data, the biggest decision is the cutpoint to use. That is, we can calculate longstring metrics and even-odd correlation, but at what value of these measures do we determine a response is bad enough to throw it out?\n\nThe truth is, we do not know for sure. These are \"unsupervised\" models: we do not have a variable that tells us if they are truly a careful responder or not. Still, we have to make a decision, and we want it to be reasonable and well justified.\n\nMy approach was to adopt cutpoints that were hard to dispute. Take for example psychometric synonyms and antonyms. Let's visualize these scores\\>:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Psychometric Synonyms/Antonyms ------------------------------------------------\n\n## set the cutoffs\ncutoff_psychsyn <- 0 # set the cutoff to screen results for psychometric synonyms\ncutoff_psychant <- 0 # set the cutoff to screen results for psychometric antonyms\n\n## calculate the metric  \ndata <- \n  data %>% \n  select(mios_1:cos_16) %>%  # select only the scale items\n  transmute(\n    psychsyn = careless::psychsyn(., critval = 0.60), # calculate the psychometric synonyms using a critical value of r = .60\n    psychant = careless::psychant(., critval = -0.60) # calculate the psychometric antonyms using a critical value of r = -.60\n  ) %>% \n  bind_cols(data) # add the psychometrics synonym/antonym scores to the data\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: There were 2 warnings in `transmute()`.\nThe first warning was:\nℹ In argument: `psychsyn = careless::psychsyn(., critval = 0.6)`.\nCaused by warning in `stats::cor()`:\n! the standard deviation is zero\nℹ Run `dplyr::last_dplyr_warnings()` to see the 1 remaining warning.\n```\n\n\n:::\n\n```{.r .cell-code}\ndata %>%\n  select(psychsyn, psychant) %>% \n  pivot_longer(everything()) %>% \n  ggplot(aes(value, fill = name)) +\n  geom_histogram() +\n  facet_wrap(~name)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-8-1.png){width=672}\n:::\n:::\n\n\n\n### Order of Operations: Putting it all together\n\nBesides setting cutpoints, a second major decision to make is how to combine different screening tools.\n\nCurran advises using a \"many small hurdles\" approach. By that, he suggests using lots of if different tools, e.g., longstring, even-odd, psychometric synonyms/antonyms -- but being lenient with the cutoff. They idea is that a lenient cutoff will only remove the responses that are most likely to be bad. In more technical terms, the small hurdles approach is about minimizing false negatives (data we throw away but shouldn't have). If we would make the hurdles more stringent, then we are likely to catch more bad data at the expense of throwing out good data.\n\nWhile the small hurdles approach makes sense, there is still little guidance about how to implement it. For example, should longstringers be removed first or last? I ask because the order of operations matters. If you remove respondents for longstringing, then calculate even-odd correlation, then the even-odd metric will be different that if the longstringers were not removed. This is especially a problem when the metrics makes use of the entire sample, such as when calculating psychometric synynonyms and antonyms. It is also a problem when using relative criteria to remove responmses. For example, by removing anyone with a longstring score 2 standard deviations above the mean.\n\nIn the end, I had to make a decision and do something that was reasonable, even if there was no precedent.\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}