{
  "hash": "829cfd2a4ce4a3e5b20c8ba747c21887",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Cleaning and Screening Messy Real-World Survey Data\"\ndescription: \"You analyze the data you have, not the data you want\"\nauthor:\n  - name: Thomas Hodges\n    orcid: 0000-0002-5184-7346\ndate: 2025-06-04\ncategories: \n  - Data Cleansing\ndraft: false\n---\n\n\n\n\n## Project Summary\n\nI have conducted dozens of surveys with employees, students, and community members. A common issue with surveys is data quality. People skip answers, they lie, they straight-line, they respond in a random pattern, they take the wrong version of surveys. I've seen it all.\n\nLuckily there are tools to screen and clean bad survey data. For this project, I used cutting-edge methods to detect poor-quality survey responses. I then repeated the process for two additional surveys.\n\nThe results are clean data sets that have been used for several scientific publications. Additionally, I am drafting a paper that outlines the methods in an accessible way.\n\nFind the complete analysis code on GitHub and a walkthrough below.\n\n## Skills Demonstrated\n\n1.  R Programming\n2.  Data Quality Assurance\n3.  Survey Design and Analysis\n\n## Real-World Applications:\n\n-   People Analytics\n\n-   Employee Health and Well-being\n\n-   Employee Engagement and Satisfaction surveys\n\n## Walkthrough\n\nWhat's the problem: Real-World\n\nLet me walk you through the problem and solution. Imagine you just completed a survey with your employees, customers, or another group of stakeholders. Here I have a survey with a group of veterans. You might be quick to import your data and start to analyze it. Using R, let's download the data and look at it:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse,quietly = TRUE)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.2     ✔ tibble    3.3.0\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.4     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\n```\n\n\n:::\n\n```{.r .cell-code}\ndata <- read_csv(\"data/veterans.csv\")\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nRows: 234 Columns: 371\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (21): StartDate, EndDate, IPAddress, RecordedDate, ResponseId, Distribu...\ndbl (343): Status, Progress, Duration (in seconds), Finished, LocationLatitu...\nlgl   (7): RecipientLastName, RecipientFirstName, RecipientEmail, ExternalRe...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n```\n\n\n:::\n\n```{.r .cell-code}\ndata %>% select(StartDate, ResponseId, pcl_1, pcl_2, pcl_3) %>% slice_head(n = 7)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 7 × 5\n  StartDate     ResponseId        pcl_1 pcl_2 pcl_3\n  <chr>         <chr>             <dbl> <dbl> <dbl>\n1 3/21/25 18:13 R_6XDY8zzzNddbmvv     1     0     0\n2 3/21/25 18:20 R_3hLIDnq6C1KR4U0     3     3     0\n3 3/21/25 18:27 R_394gR6v6FG3xC18     0     0     0\n4 3/21/25 18:24 R_7pc0ZTcbhv4yF9Q     1     0     1\n5 3/21/25 18:20 R_1doiI3icHoB6bqV     2     1     0\n6 3/21/25 18:11 R_5qadE6upLOH6fQK     4     4     3\n7 3/21/25 18:47 R_3qaVN3lueCeTLVo     0     0     0\n```\n\n\n:::\n:::\n\n\n\n\nWe have 234 rows. In this case, that's supposed to be 234 US military veterans who each responded to the survey 1 time.\n\nThe participants were recruited online, so we cannot be sure that these are all veterans. To ensure these people we veterans, we asked two questions. To participate, you have to have previously served in the military, but not currently be on active duty:\n\n1.  Did you serve in the United States military?\n\n2.  Are you now separated from the military? (e.g., discharged, released from active duty, transferred to the Inactive Ready Reserve)\n\n\n\n\n    ::: {.cell}\n    \n    ```{.r .cell-code}\n    data %>% count(military_screener)\n    ```\n    \n    ::: {.cell-output .cell-output-stdout}\n    \n    ```\n    # A tibble: 2 × 2\n      military_screener     n\n                  <dbl> <int>\n    1                 0     1\n    2                 1   233\n    ```\n    \n    \n    :::\n    \n    ```{.r .cell-code}\n    data %>% count(veteran_screener)\n    ```\n    \n    ::: {.cell-output .cell-output-stdout}\n    \n    ```\n    # A tibble: 3 × 2\n      veteran_screener     n\n                 <dbl> <int>\n    1                0    19\n    2                1   214\n    3               NA     1\n    ```\n    \n    \n    :::\n    :::\n\n\n\n\nLuckily, no one ever lies on the internet. But let's assume they did. Maybe there are people out there who spend all day looking for surveys to take. Maybe that's because some surveys offer money or compensation for responses.\n\nHow can we tell who is legit? We can build some security into the survey with a \"validity check.\" We ask a question that only a legit respondent knows. Or, we ask a series of questions and test if people are logically consistent.\n\nIn this case, we asked participants which military rank was higher, and provided them with two military ranks. For anyone in the military, it should be immediately obvious which rank is higher and lower. For people outside of the military, this is a little more difficult. not impossible, but not immediately obvious. That's perfect. We want it to be so easy that 100% of legit people can answer it accurately. This doesn't mean the liers can't answer it correctly, but it does give us one way to weed them out.\n\n> Which military rank is higher, private or lieutenant?\n\nHow many people got the answer wrong?:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata %>% \n  filter(!is.na(validity_check)) %>% \n  count(validity_check) %>% \n  mutate(perc = n / sum(n))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 3\n  validity_check     n  perc\n           <dbl> <int> <dbl>\n1              0    14  0.07\n2              1   186  0.93\n```\n\n\n:::\n:::\n\n\n\n\n14 of the 200 people who said they were veterans did not answer a simple question testing their basic military knowledge. For an online survey, that's grounds for screening your response. Let's go ahead and filter based on the screener questions and validity checks.:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata <-\n  data %>% \n  filter(\n    age_screener == 1, \n    military_screener == 1, \n    veteran_screener == 1, \n    validity_check == 1\n  )\n\ndata %>% nrow()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 186\n```\n\n\n:::\n:::\n\n\n\n\nWe can also test for logical inconsistencies or impossibilities.\n\nFor example, we might compare the rank that participants claimed to have achieved with the number of years they claimed to have served. If they report a really high rank in too few years, then they are lying about something. For example, it is VERY impressible although not impossible to reach the rank of E-7 in 7 years. This is the so-called \"7 in 7.\" Anyone who claims 7 in 7 on an online is probably lying, but they might be telling the trust. But anyone who claims to have reached 7 in fewer years is almost certainly lying. Did we have this problem in our data?\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata %>%\n  filter(pay_grade == \"E-7 to E-9\") %>% \n  ggplot(aes(years_served)) +\n  geom_histogram() + \n  labs(x = 'Years Served', y = \"n\", \"Pay Grade E-7 to E-9\") + \n  theme_minimal()\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n:::\n\n\n\n\nAs a final note, you should also check the bot detection, duplicate, and fraud stats that are available from survey hosts like Qualtrics:\n\"A score of Less than 0.5 means the respondent is likely a bot.\" https://www.qualtrics.com/support/survey-platform/survey-module/survey-checker/fraud-detection/#RelevantID\n\n\"A score greater than or equal to 30 means the response is likely fraudulent and a bot.\"\n\nA score of greater than or equal to 75 means the response is likely a duplicate. https://www.qualtrics.com/support/survey-platform/survey-module/survey-checker/fraud-detection/#RelevantID\n  )\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#Q_RecaptchaScore >= .5\n#Q_RelevantIDFraudScore < 30\n#Q_RelevantIDDuplicateScore < 75\n\ndata %>% ggplot(aes(Q_RecaptchaScore)) + geom_histogram()\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n\n```{.r .cell-code}\ndata %>% ggplot(aes(Q_RelevantIDFraudScore)) + geom_histogram()\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-3-2.png){width=672}\n:::\n\n```{.r .cell-code}\ndata %>% ggplot(aes(Q_RelevantIDDuplicateScore)) + geom_histogram()\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-3-3.png){width=672}\n:::\n:::\n\n\n\n\n\n\n\nBut a big problem with surveys is data quality. People skip answers, they lie, they straight-line, they respond in a random pattern, they take the wrong version of surveys. Depending how the data was collected, some of these problems can be worse than others.\n\nIn this case, we recruited the survey participants online. You know how easy it is to lie online. Some people probably took the survey who were not supposed to. We used a screener question to automatically kick these people out, but some may have lied. In this case, we asked two questions.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Psychometric Synonyms/Antonyms ------------------------------------------------\n\n## set the cutoffs\ncutoff_psychsyn <- 0 # set the cutoff to screen results for psychometric synonyms\ncutoff_psychant <- 0 # set the cutoff to screen results for psychometric antonyms\n\n## calculate the metric  \ndata <- \n  data %>% \n  select(mios_1:cos_16) %>%  # select only the scale items\n  transmute(\n    psychsyn = careless::psychsyn(., critval = 0.60), # calculate the psychometric synonyms using a critical value of r = .60\n    psychant = careless::psychant(., critval = -0.60) # calculate the psychometric antonyms using a critical value of r = -.60\n  ) %>% \n  bind_cols(data) # add the psychometrics synonym/antonym scores to the data\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: There were 2 warnings in `transmute()`.\nThe first warning was:\nℹ In argument: `psychsyn = careless::psychsyn(., critval = 0.6)`.\nCaused by warning in `stats::cor()`:\n! the standard deviation is zero\nℹ Run `dplyr::last_dplyr_warnings()` to see the 1 remaining warning.\n```\n\n\n:::\n\n```{.r .cell-code}\n# Longstring\n\n## Calculate longstring and average string without reverse scoring\ndata <-\n  data %>% \n  select(biis_1:cos_16) %>%         # select only the scale items after PCL and MIOS\n  transmute(\n    longstring = careless::longstring(.),\n    longstring_avg = careless::longstring(., avg = TRUE)[,2]\n  ) %>% \n  bind_cols(data)\n\n## Plot the long strings. Without the previous removals, there are 3 outliers for longstring (>15) and one for average string (>5)\ndata %>% ggplot(aes(longstring)) + geom_histogram()\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n\n```{.r .cell-code}\ndata %>% ggplot(aes(longstring_avg)) + geom_histogram()\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-4-2.png){width=672}\n:::\n\n```{.r .cell-code}\n## Remove the longstring/average string outliers\ndata <- \n  data %>% \n  filter(longstring < 15,\n         longstring_avg < 5)\n\n\n\n## Arrange the data and calculate the metric\ndata <-\n  data %>% \n  select(mios_1:cos_16) %>%         # select only the scale items\n  select(-mios_9) %>%              # issue with original did not include all 14 items\n  select(-starts_with('bipf')) %>%  # Drop the inventory\n  select(\n         # MIOS Shame\n         mios_1, \n         mios_3, \n         mios_7, \n         mios_8, \n         mios_12, \n         mios_13, \n         mios_14,\n         \n         # MIOS Trust\n         mios_2,\n         mios_4,\n         mios_5,\n         mios_6,\n         #mios_9,\n         mios_10,\n         mios_11,\n         \n         # DRDI Dysfunction\n         drdi_2,\n         drdi_4,\n         drdi_5,\n         drdi_8,\n         drdi_10,\n         drdi_13,\n         drdi_14,\n         \n         # DRDI recovery\n         drdi_1,\n         drdi_3,\n         drdi_6,\n         drdi_7,\n         drdi_9,\n         drdi_11,\n         drdi_12,\n         everything()\n  ) %>% \n  transmute(evenodd = \n              careless::evenodd(x =., \n                                # nItems in each subscale in order:\n                                factors = c(\n                                  \n                                  7,  # MIOS Shame\n                                  6,  # MIOS Trust\n                                  7,  # DRDI Dysfunction\n                                  7,  # DRDI Recovery\n                                  20, # PCL\n                                  10, # BIIS Harmony\n                                  7,  # BIIS Blended\n                                  6,  # MCARM Purpose \n                                  4,  # MCARM Help\n                                  3,  # MCARM Civilians\n                                  3,  # MCARM Resentment\n                                  5,  # MCARM Regimentation\n                                  7,  # WIS Private Regard\n                                  7,  # WIS Interdependent\n                                  3,  # WIS Connection\n                                  3,  # WIS Family\n                                  4,  # WIS Centrality\n                                  4,  # WIS Public Regard\n                                  3,  # WIS Skills\n                                \n                                 12,  # SCC \n                                  6,  # MPSS\n                                  8,  # SSS\n                                  8,  # COS\n                                  8)  # COS\n                                \n              )\n            ) %>% bind_cols(data) # Add the results back to the original data. \n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: There was 1 warning in `transmute()`.\nℹ In argument: `evenodd = careless::evenodd(...)`.\nCaused by warning in `careless::evenodd()`:\n! Computation of even-odd has changed for consistency of interpretation\n          with other indices. This change occurred in version 1.2.0. A higher\n          score now indicates a greater likelihood of careless responding. If\n          you have previously written code to cut score based on the output of\n          this function, you should revise that code accordingly.\n```\n\n\n:::\n\n```{.r .cell-code}\n## Plot the even-odd correlation. The outliers are around -.50, but that is a fairly strong correlation, which indicates careful responding. I wont remove any based on this. \ndata %>% ggplot(aes(evenodd)) + geom_histogram()\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-4-3.png){width=672}\n:::\n:::\n\n\n\n\n\n### Dirty Data: Careless survey responding. Examples using R visualizations\n\n### Cutpoints\n\n### Developing an Order of Operations\n\n## \n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}